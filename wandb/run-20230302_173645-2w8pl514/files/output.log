
before normalize 980
after normalize 980
initial torch.Size([980, 96, 96, 60])
after permute torch.Size([980, 60, 96, 96])
after 123 torch.Size([980, 25, 96, 96])
after reshape torch.Size([24500, 96, 96])
after unsqueeze torch.Size([24500, 1, 96, 96])
before normalize 490
  0%|                                                                                                          | 0/100 [00:00<?, ?it/s]
after normalize 490
initial torch.Size([490, 96, 96, 60])
after permute torch.Size([490, 60, 96, 96])
after 123 torch.Size([490, 25, 96, 96])
after reshape torch.Size([12250, 96, 96])
after unsqueeze torch.Size([12250, 1, 96, 96])
train_data.shape torch.Size([24500, 1, 96, 96])
(train_loader.dataset.__getitem__(1).shape) torch.Size([1, 96, 96])
coeffs_saved_trn.shape torch.Size([120, 200, 1, 441])
image_batches_trn.shape torch.Size([120, 200, 1, 32, 32])
coeffs_saved_test.shape torch.Size([20, 200, 1, 441])
image_batches_test.shape torch.Size([20, 200, 1, 32, 32])
coeffs_saved_trn.max() tensor(2.3992, device='cuda:0', dtype=torch.float64)
  0%|                                                                                                          | 0/100 [00:00<?, ?it/s]/home/ramana44/autoencoder-regularisation-/train_ae_LSTSQ20_noSamples100degPoly55.py:274: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180549130/work/aten/src/ATen/native/LinearAlgebra.cpp:464.)
  rank_Jacm = float(torch.matrix_rank(Jac_m, tol=1e-1).cpu().detach().numpy())
[0] rAE rank = 6, cond = 4.2202e+01
[0] rAE rank = 6, cond = 2.4236e+01
[0] rAE rank = 6, cond = 2.3498e+01
[0] rAE rank = 7, cond = 4.2237e+01
[0] rAE rank = 7, cond = 1.0898e+02
[0] rAE rank = 8, cond = 1.6397e+02
[0] rAE rank = 8, cond = 3.1996e+01
[0] rAE rank = 8, cond = 1.7781e+01
[0] rAE rank = 8, cond = 1.2980e+01
[0] rAE rank = 8, cond = 1.0419e+01
[0] rAE rank = 9, cond = 8.7710e+00
  0%|                                                                                                          | 0/100 [00:39<?, ?it/s]
Traceback (most recent call last):
  File "/home/ramana44/autoencoder-regularisation-/training_call_Hybrid_AEs_FashionMNIST/LSTQS20_TDA_40_AlphaPt5_lat10.py", line 108, in <module>
    model, model_reg, loss_arr_reg, loss_arr_reco, loss_arr_base, loss_arr_val_reco, loss_arr_val_base = train(train_loader, test_loader, no_epochs=100, reco_loss="mse", latent_dim=latent_dim,
  File "/home/ramana44/autoencoder-regularisation-/train_ae_LSTSQ20_noSamples100degPoly55.py", line 236, in train
    loss_C1, Jac = computeC1Loss_upd(nodes_subsample, model_reg, device, guidanceTerm = use_guidance) # guidance term
  File "/home/ramana44/autoencoder-regularisation-/regularisers_without_vegas_fmnist.py", line 182, in computeC1Loss_upd
    Jac = torch.autograd.functional.jacobian(f, node_points.to(device), create_graph = True).squeeze() # compute Jacobian
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/functional.py", line 579, in jacobian
    vj = _autograd_grad((out.reshape(-1)[j],), inputs,
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/functional.py", line 147, in _autograd_grad
    return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
KeyboardInterrupt