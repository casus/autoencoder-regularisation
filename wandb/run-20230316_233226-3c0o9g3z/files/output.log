before normalize 123
after normalize 123
initial torch.Size([123, 96, 96, 60])
after permute torch.Size([123, 60, 96, 96])
after 123 torch.Size([123, 25, 96, 96])
after reshape torch.Size([3075, 96, 96])
after unsqueeze torch.Size([3075, 1, 96, 96])
before normalize 490
after normalize 490
initial torch.Size([490, 96, 96, 60])
after permute torch.Size([490, 60, 96, 96])
after 123 torch.Size([490, 25, 96, 96])
after reshape torch.Size([12250, 96, 96])
after unsqueeze torch.Size([12250, 1, 96, 96])
train_data.shape torch.Size([3075, 1, 96, 96])
  0%|                                                                                                           | 0/50 [00:00<?, ?it/s]/home/ramana44/autoencoder-regularisation-/training_call_Hybrid_AEs_MRI/train_ae_n.py:236: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180549130/work/aten/src/ATen/native/LinearAlgebra.cpp:464.)
  rank_Jacm = float(torch.matrix_rank(Jac_m, tol=1e-1).cpu().detach().numpy())
[0] rAE rank = 10, cond = 1.9056e+00
[0] rAE rank = 10, cond = 2.2192e+00
[0] rAE rank = 10, cond = 2.0449e+00
[0] rAE rank = 10, cond = 2.1794e+00
  2%|█▉                                                                                                 | 1/50 [00:08<07:20,  8.99s/it]
[0] rAE rank = 10, cond = 2.8999e+00
[0] rAE rank = 10, cond = 2.7827e+00
[0] rAE loss = 4.9046e-03, rAE reconstruction loss = 3.3302e-03
[1] rAE rank = 10, cond = 2.2666e+00
[1] rAE rank = 10, cond = 1.7771e+00
[1] rAE rank = 10, cond = 1.7354e+00

  4%|███▉                                                                                               | 2/50 [00:17<06:45,  8.45s/it]
[1] rAE rank = 10, cond = 1.6690e+00
[1] rAE rank = 10, cond = 1.6014e+00
[1] rAE loss = 3.1651e-03, rAE reconstruction loss = 2.4126e-03
[2] rAE rank = 10, cond = 1.5849e+00
[2] rAE rank = 10, cond = 1.8109e+00
[2] rAE rank = 10, cond = 1.7896e+00

  6%|█████▉                                                                                             | 3/50 [00:24<06:19,  8.08s/it]
[2] rAE rank = 10, cond = 1.5566e+00
[2] rAE rank = 10, cond = 1.8044e+00
[2] rAE loss = 2.9181e-03, rAE reconstruction loss = 2.0755e-03
[3] rAE rank = 10, cond = 1.6058e+00
[3] rAE rank = 10, cond = 1.6382e+00
[3] rAE rank = 10, cond = 1.6325e+00
[3] rAE rank = 10, cond = 1.6681e+00

  8%|███████▉                                                                                           | 4/50 [00:32<06:06,  7.97s/it]
[3] rAE rank = 10, cond = 1.5941e+00
[3] rAE loss = 2.7915e-03, rAE reconstruction loss = 1.9589e-03
[4] rAE rank = 10, cond = 1.5889e+00
  8%|███████▉                                                                                           | 4/50 [00:36<06:56,  9.06s/it]
Traceback (most recent call last):
  File "/home/ramana44/autoencoder-regularisation-/training_call_Hybrid_AEs_MRI/mrt_train_testAlphaPt1Lat10.py", line 118, in <module>
    model, model_reg, loss_arr_reg, loss_arr_reco, loss_arr_base, loss_arr_val_reco, loss_arr_val_base = train(image_batches_trn, image_batches_test, coeffs_saved_trn, coeffs_saved_test, no_epochs=50, reco_loss="mse", latent_dim=latent_dim,
  File "/home/ramana44/autoencoder-regularisation-/training_call_Hybrid_AEs_MRI/train_ae_n.py", line 253, in train
    _, Jac = computeC1Loss(Jac_val_pts, model, device)
  File "/home/ramana44/autoencoder-regularisation-/regularisers_without_vegas_fmnist.py", line 161, in computeC1Loss
    Jac = torch.autograd.functional.jacobian(f, cheb_nodes.to(device), create_graph = True).squeeze() # compute Jacobian
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/functional.py", line 579, in jacobian
    vj = _autograd_grad((out.reshape(-1)[j],), inputs,
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/functional.py", line 147, in _autograd_grad
    return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
KeyboardInterrupt
[4] rAE rank = 10, cond = 1.5457e+00