
before normalize 245
after normalize 245
initial torch.Size([245, 96, 96, 60])
after permute torch.Size([245, 60, 96, 96])
after 123 torch.Size([245, 25, 96, 96])
after reshape torch.Size([6125, 96, 96])
after unsqueeze torch.Size([6125, 1, 96, 96])
before normalize 490
after normalize 490
initial torch.Size([490, 96, 96, 60])
after permute torch.Size([490, 60, 96, 96])
after 123 torch.Size([490, 25, 96, 96])
after reshape torch.Size([12250, 96, 96])
after unsqueeze torch.Size([12250, 1, 96, 96])
train_data.shape torch.Size([6125, 1, 96, 96])
(train_loader.dataset.__getitem__(1).shape) torch.Size([1, 96, 96])
  0%|                                                                                                          | 0/100 [00:00<?, ?it/s]
coeffs_saved_trn.shape torch.Size([30, 200, 1, 441])
image_batches_trn.shape torch.Size([30, 200, 1, 32, 32])
coeffs_saved_test.shape torch.Size([5, 200, 1, 441])
image_batches_test.shape torch.Size([5, 200, 1, 32, 32])
coeffs_saved_trn.max() tensor(1.9227, device='cuda:0', dtype=torch.float64)
  0%|                                                                                                          | 0/100 [00:00<?, ?it/s]/home/ramana44/autoencoder-regularisation-/train_ae_LSTSQ20_noSamples100degPoly55.py:274: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180549130/work/aten/src/ATen/native/LinearAlgebra.cpp:464.)
  rank_Jacm = float(torch.matrix_rank(Jac_m, tol=1e-1).cpu().detach().numpy())
[0] rAE rank = 1, cond = 1.3811e+02
[0] rAE rank = 2, cond = 4.2834e+00
[0] rAE rank = 2, cond = 2.5218e+00
[0] rAE rank = 2, cond = 1.8414e+00
[0] rAE rank = 2, cond = 1.4422e+00
[0] rAE rank = 2, cond = 1.2041e+00
[0] rAE rank = 2, cond = 1.0556e+00
[0] rAE rank = 2, cond = 1.0877e+00
[0] rAE rank = 2, cond = 1.1719e+00
[0] rAE rank = 2, cond = 1.2435e+00
[0] rAE rank = 2, cond = 1.2828e+00
[0] rAE rank = 2, cond = 1.2969e+00
[0] rAE rank = 2, cond = 1.2856e+00
  0%|                                                                                                          | 0/100 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/home/ramana44/autoencoder-regularisation-/training_call_Hybrid_AEs_FashionMNIST/LSTQS20_TDA_40_AlphaPt5_lat2.py", line 108, in <module>
    model, model_reg, loss_arr_reg, loss_arr_reco, loss_arr_base, loss_arr_val_reco, loss_arr_val_base = train(train_loader, test_loader, no_epochs=100, reco_loss="mse", latent_dim=latent_dim,
  File "/home/ramana44/autoencoder-regularisation-/train_ae_LSTSQ20_noSamples100degPoly55.py", line 291, in train
    _, Jac = computeC1Loss(Jac_val_pts, model, device)
  File "/home/ramana44/autoencoder-regularisation-/regularisers_without_vegas_fmnist.py", line 161, in computeC1Loss
    Jac = torch.autograd.functional.jacobian(f, cheb_nodes.to(device), create_graph = True).squeeze() # compute Jacobian
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/functional.py", line 579, in jacobian
    vj = _autograd_grad((out.reshape(-1)[j],), inputs,
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/functional.py", line 147, in _autograd_grad
    return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,
  File "/home/ramana44/.conda/envs/myenv/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
KeyboardInterrupt